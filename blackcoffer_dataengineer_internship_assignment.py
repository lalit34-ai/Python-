# -*- coding: utf-8 -*-
"""blackcoffer_dataengineer_internship_assignment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12s2E5k_oXGosAWLYINucpgOWWRuL357O
"""

import pandas as pd
from bs4 import BeautifulSoup

import requests







def get_article_data(url):
  response = requests.get(url,headers={"User-Agent": "XY"})
  soup = BeautifulSoup(response.content)
  final = ""
  for tag in soup.find("div",{"class":"td-post-content"}).find_all("p",recursive=False):
    final = final + " " + tag.text
  return final

df_input = pd.read_csv("/content/Input.xlsx - Sheet1.csv")

df_input.shape

df_input.head()

get_article_data(df_input["URL"][0])

!mkdir articles

for row in df_input.values:
  text = get_article_data(row[1])
  #open text file
  text_file = open("/content/articles/" + str(row[0]) +".txt", "w")
  #write string to file
  text_file.write(text)
  #close file
  text_file.close()
  print(row[0])

!zip -r /content/articles.zip /content/articles/

from google.colab import files
files.download("/content/articles.zip")

df_output = pd.read_csv("/content/Output Data Structure.xlsx - Sheet1.csv")

df_output.columns



stopwords_given = []
file_stopwords = open("/content/StopWords_Names.txt","r")
lines = file_stopwords.readlines()
for line in lines:
  stopwords_given.append(line.strip().split()[0].lower())
file_stopwords.close()
file_stopwords = open("/content/StopWords_Auditor.txt","r")
lines = file_stopwords.readlines()
for line in lines:
  stopwords_given.append(line.strip().split()[0].lower())
file_stopwords.close()
file_stopwords = open("/content/StopWords_Currencies.txt","r", encoding='utf-8', errors='ignore')
lines = file_stopwords.readlines()
for line in lines:
  stopwords_given.append(line.strip().split()[0].lower())
file_stopwords.close()
file_stopwords = open("/content/StopWords_DatesandNumbers.txt","r")
lines = file_stopwords.readlines()
for line in lines:
  stopwords_given.append(line.strip().split()[0].lower())
file_stopwords.close()
file_stopwords = open("/content/StopWords_GenericLong.txt","r")
lines = file_stopwords.readlines()
for line in lines:
  stopwords_given.append(line.strip().split()[0].lower())
file_stopwords.close()
file_stopwords = open("/content/StopWords_Geographic.txt","r")
lines = file_stopwords.readlines()
for line in lines:
  stopwords_given.append(line.strip().split()[0].lower())
file_stopwords.close()
file_stopwords = open("/content/StopWords_Generic.txt","r")
lines = file_stopwords.readlines()
for line in lines:
  stopwords_given.append(line.strip().split()[0].lower())
file_stopwords.close()
print("stopwords_given lenght",len(stopwords_given))
stopwords_given = list(set(stopwords_given))
print("unique stop words",len(stopwords_given))
positive_words = []
file_positive = open("/content/positive-words.txt","r")
lines = file_positive.readlines()
for line in lines:
  positive_words.append(line.strip().split()[0].lower())
file_positive.close()
print("positive words length",len(positive_words))
postive_words = [word for word in positive_words if word not in stopwords_given]
print("length of positve words",len(positive_words))
negative_words = []
file_negative = open("/content/negative-words.txt","r", encoding='utf-8', errors='ignore')
lines = file_negative.readlines()
for line in lines:
  negative_words.append(line.strip().split()[0].lower())
file_negative.close()
print("negative words length",len(negative_words))
negative_words = [word for word in negative_words if word not in stopwords_given]
print("length of positve words",len(negative_words))



df_input.head()

df_output.columns

def get_positive_score(text):
  text = text.lower()
  count = 0
  for word in text.split():
    if(word in positive_words):
      count+=1
  return count

def get_negative_score(text):
  text = text.lower()
  count = 0
  for word in text.split():
      if(word in negative_words):
        count+=1
  return count

get_negative_score(get_article_data(df_input["URL"][0]))

get_positive_score(get_article_data(df_input["URL"][0]))

import nltk
nltk.download('stopwords')
nltk.download("punkt")
nltk.download('tagsets')
nltk.download('averaged_perceptron_tagger')

from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
stop_words = stopwords.words('english')
stop_words = stop_words + stopwords_given

df_output["raw_text"] = df_output["URL"].apply(get_article_data)

def clean(text):
  tokens = word_tokenize(text)
  words = [word for word in tokens if word.isalpha()]
  words = [w for w in words if not w in stop_words]
  return " ".join(words)

df_output["text"] = df_output["raw_text"].apply(clean)



df_output["POSITIVE SCORE"] = df_output["text"].apply(get_positive_score)

df_output["NEGATIVE SCORE"] = df_output["text"].apply(get_negative_score)

df_output["POLARITY SCORE"]  = (df_output["POSITIVE SCORE"] - df_output["NEGATIVE SCORE"])/(df_output["POSITIVE SCORE"] + df_output["NEGATIVE SCORE"] + 0.000001)

df_output.columns

df_output["SUBJECTIVITY SCORE"] = (df_output["POSITIVE SCORE"] + df_output["NEGATIVE SCORE"])/(0.000001 + len(df_output["text"]))

df_output.columns



import spacy

nlp = spacy.load("en_core_web_sm")



def get_avg_sentence_length(text):
  number_of_words = len(text.split())
  number_of_sentences = len([sent.text for sent in nlp(text).sents])
  return number_of_words/number_of_sentences

df_output["AVG SENTENCE LENGTH"] = df_output["raw_text"].apply(get_avg_sentence_length)

!pip install syllables

import syllables
syllables.estimate('estimate')

df_output.columns

def get_percentage_of_complex_words(text):
  number_of_words = len(text.split())
  number_of_complex_words = len([word for word in text.split() if syllables.estimate(word)>2])
  return number_of_complex_words/number_of_words

df_output["PERCENTAGE OF COMPLEX WORDS"] = df_output["text"].apply(get_percentage_of_complex_words)

df_output["FOG INDEX"] = 0.4*(df_output["AVG SENTENCE LENGTH"] + df_output["PERCENTAGE OF COMPLEX WORDS"])

df_output["AVG NUMBER OF WORDS PER SENTENCE"] = df_output["raw_text"].apply(get_avg_sentence_length)

df_output["COMPLEX WORD COUNT"]   = df_output["text"].apply(lambda x: len([word for word in x.split() if syllables.estimate(word)>2]))

df_output["WORD COUNT"] = df_output["text"].apply(lambda x:len(x.split()))

df_output["SYLLABLE PER WORD"] = df_output["text"].apply(lambda x:sum([syllables.estimate(word) for word in x]))

def get_count_of_personal_pronouns(text):
  return len([word[0] for word in nltk.pos_tag(text.split()) if word[1] == "PRP"])

df_output["PERSONAL PRONOUNS"] = df_output["raw_text"].apply(get_count_of_personal_pronouns)

def get_average_word_length(text):
  sum = len(text.replace(" ",""))
  return sum/len(text.split())

df_output["AVG WORD LENGTH"] = df_output["text"].apply(get_average_word_length)

df_output = df_output.drop(columns=["AVG NUMBER OF WORDS PER SCENTENCE","raw_text","text"],axis=1)

df_output.shape

df_output.to_csv("output.csv",index=False)

df_output.head()

